---
title: "An√°lisis de Flujo de Datos Simulado con Spark y Jekyll"
layout: default
---

# üí° An√°lisis de Flujo de Datos Simulado con Spark y Jekyll

## üéØ Objetivo
Aplicar anal√≠tica avanzada para procesar un flujo de datos simulado en un contexto empresarial usando Python y Apache Spark, demostrando el poder del procesamiento distribuido y el an√°lisis en tiempo real.

---

## üõçÔ∏è Escenario: Tienda Online y An√°lisis de Clics
Imaginemos una tienda online que desea analizar en tiempo real los clics de los usuarios dentro de su sitio web.  
El objetivo es detectar patrones de navegaci√≥n, medir el nivel de inter√©s de los clientes y optimizar campa√±as publicitarias o recomendaciones de productos.

### üìÇ Dataset simulado
Se gener√≥ un conjunto de datos con las siguientes columnas:

- Timestamp: Fecha y hora del clic.  
- User_ID: Identificador del usuario.  
- Clicks: N√∫mero de clics realizados en esa franja temporal.  

El dataset contiene 1000 registros simulados, representando clics en distintas ventanas de tiempo.

---

## ‚öôÔ∏è Proceso de Implementaci√≥n con Spark

### 1Ô∏è‚É£ Configuraci√≥n del Entorno
Se instal√≥ Apache Spark 3.5.0 en Google Colab y se configur√≥ el entorno de ejecuci√≥n de Java y las variables SPARK_HOME y JAVA_HOME.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ClickstreamAnalysis").getOrCreate()
print("Versi√≥n de Spark:", spark.version)
2Ô∏è‚É£ Carga y Exploraci√≥n de Datos
El archivo CSV se carg√≥ en un DataFrame de Spark:

python
Copiar c√≥digo
df = spark.read.csv("clickstream_data.csv", header=True, inferSchema=True)
df.show(5)
Esto permite trabajar con grandes vol√∫menes de datos de manera eficiente.

3Ô∏è‚É£ Procesamiento del Flujo
Se simul√≥ un flujo de clics usando ventanas de 1 minuto, agrupando los clics por usuario:

python
Copiar c√≥digo
from pyspark.sql.functions import sum

clicks_per_user = df.groupBy("User_ID").agg(sum("Clicks").alias("Total_Clicks"))
clicks_per_user.show()
El procesamiento en ventanas es una de las caracter√≠sticas que diferencia a Spark Streaming del procesamiento por lotes, ya que permite analizar los datos en tiempo real.

4Ô∏è‚É£ Visualizaci√≥n
Con los datos procesados, se cre√≥ una gr√°fica de barras mostrando los clics por usuario utilizando matplotlib:

python
Copiar c√≥digo
import matplotlib.pyplot as plt

pdf = clicks_per_user.toPandas()
plt.bar(pdf["User_ID"], pdf["Total_Clicks"])
plt.xlabel("Usuarios")
plt.ylabel("Clics Totales")
plt.title("Clics por Usuario en Ventanas de 1 Minuto")
plt.show()

üìä Interpretaci√≥n Anal√≠tica
El an√°lisis muestra que algunos usuarios presentan una cantidad de clics significativamente mayor que otros.
Esto puede indicar:

Mayor nivel de interacci√≥n o inter√©s en los productos.

Posibles usuarios recurrentes o fieles.

Oportunidad de aplicar estrategias de marketing personalizado para retenerlos.

Adem√°s, los usuarios con poca actividad pueden representar visitantes ocasionales o clientes potenciales que necesitan incentivos adicionales para completar una compra.

üß± Arquitectura y Despliegue del Blog
üß© Arquitectura
El proyecto se estructura de la siguiente forma:

Google Colab + PySpark: procesamiento y visualizaci√≥n de los datos.

Jekyll: generaci√≥n del blog est√°tico.

GitHub Pages: alojamiento gratuito en la nube.

Markdown (.md): formato usado para documentar el proceso y resultados.

üöÄ Despliegue del Blog
Se cre√≥ el repositorio pablo34r.github.io en GitHub.

Se activ√≥ GitHub Pages y se seleccion√≥ el tema Cayman.

Este archivo Markdown se coloc√≥ en la ra√≠z del repositorio.

Se guardaron los gr√°ficos generados (.png) en /assets/images/.

El blog se actualiza autom√°ticamente cada vez que se realiza un push al repositorio.

üîÑ Diferencias entre Streaming y Procesamiento por Lotes
Procesamiento por Lotes	Procesamiento en Streaming
Analiza datos acumulados y almacenados.	Analiza datos en tiempo real mientras llegan.
Mayor latencia.	Baja latencia y decisiones inmediatas.
Ejemplo: ventas diarias.	Ejemplo: clics por segundo.

El streaming permite detectar comportamientos instant√°neos, lo cual es esencial en 2025 para empresas que buscan reaccionar de forma √°gil ante sus clientes.

üß† Cierre y Reflexi√≥n
Este ejercicio permiti√≥ aplicar anal√≠tica avanzada con Spark, demostrando c√≥mo el procesamiento en streaming puede aportar valor inmediato a un negocio digital.
El uso de herramientas como Spark, Python y Jekyll facilita la integraci√≥n de la anal√≠tica con la publicaci√≥n de resultados en la web.

¬© 2025 - Blog de Pablo Roncancio
